{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Austen and Melville sentences...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 1 (COMPLETE)\n",
    "print(\"1. Loading Austen and Melville sentences...\")\n",
    "a_sents_all = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "m_sents_all = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Discarding short sentences and labeling...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 2\n",
    "print(\"2. Discarding short sentences and labeling...\")\n",
    "a_sents = [(s, 'austen') for s in a_sents_all if len(s)>2]\n",
    "m_sents = [(s, 'melville') for s in m_sents_all if len(s)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Joining the two author sentence lists...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 3\n",
    "print(\"3. Joining the two author sentence lists...\")\n",
    "sents = a_sents + m_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Sentence stats:\n",
      " # of total sentences: 17152\n",
      " # of Austen sentences: 7563\n",
      " # of Melville sentences: 9589\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 4\n",
    "print(\"4. Sentence stats:\")\n",
    "print(\" # of total sentences:\", len(sents))\n",
    "print(\" # of Austen sentences:\", len(a_sents))\n",
    "print(\" # of Melville sentences:\", len(m_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Shuffling...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 5\n",
    "print(\"5. Shuffling...\")\n",
    "random.Random(42).shuffle(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Partitioning...\n",
      " # of test sentences: 1000\n",
      " # of devtest sentences: 1000\n",
      " # of training sentences: 15152\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 6\n",
    "print(\"6. Partitioning...\") \n",
    "test_sents = sents[:1000]     \n",
    "devtest_sents = sents[1000:2000]  \n",
    "train_sents = sents[2000:]    \n",
    "\n",
    "print(\" # of test sentences:\", len(test_sents))\n",
    "print(\" # of devtest sentences:\", len(devtest_sents))\n",
    "print(\" # of training sentences:\", len(train_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Defining a feature-generator function...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 7\n",
    "print(\"7. Defining a feature-generator function...\")\n",
    "mainchars = {'Emma', 'Harriet', 'Ahab', 'Weston', 'Knightley', 'Elton',\n",
    "             'Woodhouse', 'Jane', 'Stubb', 'Queequeg', 'Fairfax', 'Churchill',\n",
    "             'Frank', 'Starbuck', 'Pequod', 'Hartfield', 'Bates', 'Highbury',\n",
    "             'Perry', 'Bildad', 'Peleg', 'Pip', 'Cole', 'Goddard',\n",
    "             'Campbell', 'Donwell', 'Dixon', 'Taylor', 'Tashtego'}\n",
    "\n",
    "noCharNames = False    # For [PART B] Q3\n",
    "if noCharNames :\n",
    "    print('NOTE: Top 35 proper nouns have been neutralized.') \n",
    "\n",
    "def gen_feats(sent):\n",
    "    featdict = {}\n",
    "    for w in sent:\n",
    "        if noCharNames == True:\n",
    "            if w in mainchars: w = 'MontyPython'\n",
    "        featdict['contains-'+w.lower()] = 1\n",
    "    return featdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------ [PART B] Q3\n",
    "\n",
    "noCharNames2 = True    # For [PART B] Q3\n",
    "if noCharNames :\n",
    "    print('NOTE: Top 35 proper nouns have been neutralized.') \n",
    "\n",
    "def gen_feats2(sent):\n",
    "    featdict = {}\n",
    "    for w in sent:\n",
    "        if noCharNames2 == True:\n",
    "            if w in mainchars: w = 'MontyPython'\n",
    "        featdict['contains-'+w.lower()] = 1\n",
    "    return featdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Generating feature sets...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 8\n",
    "print(\"8. Generating feature sets...\")\n",
    "test_feats = [(gen_feats(n), author) for (n,author) in test_sents]\n",
    "devtest_feats = [(gen_feats(n), author) for (n,author) in devtest_sents]  \n",
    "train_feats = [(gen_feats(n), author) for (n,author) in train_sents]\n",
    "\n",
    "## For [PART B] Q3\n",
    "test_feats2 = [(gen_feats2(n), author) for (n,author) in test_sents]\n",
    "devtest_feats2 = [(gen_feats2(n), author) for (n,author) in devtest_sents]  \n",
    "train_feats2 = [(gen_feats2(n), author) for (n,author) in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Training...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 9\n",
    "print(\"9. Training...\")\n",
    "whosaid = nltk.NaiveBayesClassifier.train(train_feats)\n",
    "\n",
    "## For [PART B] Q3\n",
    "whosaid2 = nltk.NaiveBayesClassifier.train(train_feats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. Testing...\n",
      " Accuracy score: 0.942\n",
      " Accuracy score for PARTB Q3: 0.931\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 10\n",
    "print(\"10. Testing...\")\n",
    "accuracy = nltk.classify.accuracy(whosaid, test_feats) \n",
    "print(\" Accuracy score:\", accuracy)\n",
    "\n",
    "## For [PART B] Q3\n",
    "accuracy2 = nltk.classify.accuracy(whosaid2, test_feats2) \n",
    "print(\" Accuracy score for PARTB Q3:\", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. Sub-dividing development testing set...\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 11\n",
    "print(\"11. Sub-dividing development testing set...\")\n",
    "\n",
    "# aa: real author Austen, guessed Austen\n",
    "# mm: real author Melville, guessed Melville\n",
    "# am: real author Austen, guessed Melville\n",
    "# ma: real author Melville, guessed Austen\n",
    "\n",
    "aa, mm, am, ma = [], [], [], []\n",
    "for (sent, auth) in devtest_sents:\n",
    "    guess = whosaid.classify(gen_feats(sent))\n",
    "    if auth == 'austen' and guess == 'austen':\n",
    "        aa.append( (auth, guess, sent) )\n",
    "    elif auth == 'melville' and guess == 'melville':\n",
    "        mm.append( (auth, guess, sent) )\n",
    "    elif auth == 'austen' and guess == 'melville':\n",
    "        am.append( (auth, guess, sent) )\n",
    "    else:\n",
    "        ma.append( (auth, guess, sent) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12. Sample CORRECT and INCORRECT predictions from dev-test set:\n",
      "-------\n",
      "REAL=austen   GUESS=austen  \n",
      "By birth she belonged to Highbury : and when at three years old , on losing her mother , she became the property , the charge , the consolation , the fondling of her grandmother and aunt , there had seemed every probability of her being permanently fixed there ; of her being taught only what very limited means could command , and growing up with no advantages of connexion or improvement , to be engrafted on what nature had given her in a pleasing person , good understanding , and warm - hearted , well - meaning relations .\n",
      "-------\n",
      "REAL=melville GUESS=melville\n",
      "Come , then , to my cabin .\n",
      "-------\n",
      "REAL=austen   GUESS=melville\n",
      "They are ripening fast .\"\n",
      "-------\n",
      "REAL=melville GUESS=austen  \n",
      "Oh , my sweet cardinals !\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 12\n",
    "print(\"12. Sample CORRECT and INCORRECT predictions from dev-test set:\")\n",
    "print(\"-------\")\n",
    "for x in (aa, mm ,am, ma):\n",
    "    auth, guess, sent = random.choice(x)\n",
    "    print('REAL=%-8s GUESS=%-8s' % (auth, guess))  # string formatting\n",
    "    print(' '.join(sent))\n",
    "    print(\"-------\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13. Looking up 40 most informative features...\n",
      "Most Informative Features\n",
      "           contains-miss = 1              austen : melvil =    385.9 : 1.0\n",
      "          contains-frank = 1              austen : melvil =    157.2 : 1.0\n",
      "           contains-thou = 1              melvil : austen =    103.1 : 1.0\n",
      "        contains-captain = 1              melvil : austen =     86.7 : 1.0\n",
      "            contains-mrs = 1              austen : melvil =     67.2 : 1.0\n",
      "          contains-smith = 1              austen : melvil =     64.0 : 1.0\n",
      "             contains-ye = 1              melvil : austen =     59.4 : 1.0\n",
      "         contains-martin = 1              austen : melvil =     58.0 : 1.0\n",
      "           contains-feet = 1              melvil : austen =     50.6 : 1.0\n",
      "       contains-isabella = 1              austen : melvil =     49.6 : 1.0\n",
      "           contains-fish = 1              melvil : austen =     42.3 : 1.0\n",
      "      contains-agreeable = 1              austen : melvil =     41.1 : 1.0\n",
      "           contains-dear = 1              austen : melvil =     40.0 : 1.0\n",
      "        contains-herself = 1              austen : melvil =     38.7 : 1.0\n",
      "           contains-iron = 1              melvil : austen =     38.0 : 1.0\n",
      "        contains-manners = 1              austen : melvil =     37.7 : 1.0\n",
      "      contains-happiness = 1              austen : melvil =     33.8 : 1.0\n",
      "             contains-\"' = 1              melvil : austen =     33.3 : 1.0\n",
      "         contains-nobody = 1              austen : melvil =     33.3 : 1.0\n",
      "         contains-waters = 1              melvil : austen =     30.7 : 1.0\n",
      "     contains-delightful = 1              austen : melvil =     30.1 : 1.0\n",
      "       contains-carriage = 1              austen : melvil =     29.7 : 1.0\n",
      "       contains-daughter = 1              austen : melvil =     29.7 : 1.0\n",
      "            contains-thy = 1              melvil : austen =     28.2 : 1.0\n",
      "   contains-acquaintance = 1              austen : melvil =     27.7 : 1.0\n",
      "             contains-st = 1              melvil : austen =     27.5 : 1.0\n",
      "       contains-pleasure = 1              austen : melvil =     27.5 : 1.0\n",
      "           contains-.--\" = 1              austen : melvil =     26.7 : 1.0\n",
      "        contains-further = 1              melvil : austen =     26.0 : 1.0\n",
      "           contains-dead = 1              melvil : austen =     25.7 : 1.0\n",
      "          contains-perry = 1              austen : melvil =     24.1 : 1.0\n",
      "      contains-delighted = 1              austen : melvil =     24.1 : 1.0\n",
      "          contains-marry = 1              austen : melvil =     24.1 : 1.0\n",
      "             contains-,) = 1              austen : melvil =     24.1 : 1.0\n",
      "            contains-sea = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-green = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-maple = 1              austen : melvil =     22.5 : 1.0\n",
      "      contains-gratitude = 1              austen : melvil =     22.5 : 1.0\n",
      "         contains-robert = 1              austen : melvil =     22.5 : 1.0\n",
      "          contains-along = 1              melvil : austen =     22.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------ STEP 13\n",
    "print(\"13. Looking up 40 most informative features...\")\n",
    "whosaid.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifier Accuracy\n",
    "#### What is the system's accuracy? Is it lower or higher than you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test data: 0.942\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(whosaid, test_feats) \n",
    "print(\"Accuracy score on test data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score on test data is 94.2% which is surprising high considering how models deployed in companies or real world applications are considered to be functional if  the accuracy score is over 60-70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features\n",
    "#### Examine the gen_feats() function. What sort of features is used in this classifier model? Examine the list of the most informative features and make observations. Do you notice any patterns? Any surprising entries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up 40 most informative features...\n",
      "Most Informative Features\n",
      "           contains-miss = 1              austen : melvil =    385.9 : 1.0\n",
      "          contains-frank = 1              austen : melvil =    157.2 : 1.0\n",
      "           contains-thou = 1              melvil : austen =    103.1 : 1.0\n",
      "        contains-captain = 1              melvil : austen =     86.7 : 1.0\n",
      "            contains-mrs = 1              austen : melvil =     67.2 : 1.0\n",
      "          contains-smith = 1              austen : melvil =     64.0 : 1.0\n",
      "             contains-ye = 1              melvil : austen =     59.4 : 1.0\n",
      "         contains-martin = 1              austen : melvil =     58.0 : 1.0\n",
      "           contains-feet = 1              melvil : austen =     50.6 : 1.0\n",
      "       contains-isabella = 1              austen : melvil =     49.6 : 1.0\n",
      "           contains-fish = 1              melvil : austen =     42.3 : 1.0\n",
      "      contains-agreeable = 1              austen : melvil =     41.1 : 1.0\n",
      "           contains-dear = 1              austen : melvil =     40.0 : 1.0\n",
      "        contains-herself = 1              austen : melvil =     38.7 : 1.0\n",
      "           contains-iron = 1              melvil : austen =     38.0 : 1.0\n",
      "        contains-manners = 1              austen : melvil =     37.7 : 1.0\n",
      "      contains-happiness = 1              austen : melvil =     33.8 : 1.0\n",
      "             contains-\"' = 1              melvil : austen =     33.3 : 1.0\n",
      "         contains-nobody = 1              austen : melvil =     33.3 : 1.0\n",
      "         contains-waters = 1              melvil : austen =     30.7 : 1.0\n",
      "     contains-delightful = 1              austen : melvil =     30.1 : 1.0\n",
      "       contains-carriage = 1              austen : melvil =     29.7 : 1.0\n",
      "       contains-daughter = 1              austen : melvil =     29.7 : 1.0\n",
      "            contains-thy = 1              melvil : austen =     28.2 : 1.0\n",
      "   contains-acquaintance = 1              austen : melvil =     27.7 : 1.0\n",
      "             contains-st = 1              melvil : austen =     27.5 : 1.0\n",
      "       contains-pleasure = 1              austen : melvil =     27.5 : 1.0\n",
      "           contains-.--\" = 1              austen : melvil =     26.7 : 1.0\n",
      "        contains-further = 1              melvil : austen =     26.0 : 1.0\n",
      "           contains-dead = 1              melvil : austen =     25.7 : 1.0\n",
      "          contains-perry = 1              austen : melvil =     24.1 : 1.0\n",
      "      contains-delighted = 1              austen : melvil =     24.1 : 1.0\n",
      "          contains-marry = 1              austen : melvil =     24.1 : 1.0\n",
      "             contains-,) = 1              austen : melvil =     24.1 : 1.0\n",
      "            contains-sea = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-green = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-maple = 1              austen : melvil =     22.5 : 1.0\n",
      "      contains-gratitude = 1              austen : melvil =     22.5 : 1.0\n",
      "         contains-robert = 1              austen : melvil =     22.5 : 1.0\n",
      "          contains-along = 1              melvil : austen =     22.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Looking up 40 most informative features...\")\n",
    "whosaid.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gen_feats( ) function has a list of main character names and whenever that name appears in a sentence, it assigns a value of 1 to that character name key in the feature dictionary. Each of the feature represents a dummy variable indicating whether it contains a certain main character's name (=1) or not (=0). When we look at the top40 most informative features, we see that words thats are associated with females such as \"miss\", \"mrs\" and \"dear\" and words that represent emotions such as \"gratitude\" and \"delighted\" are (depending on the word) 22 to 380 times more likely be said by Austen than Melville. On the other hand, male associated words like \"captain\" and words that refer to specific entities (e.g. animals, nature, objects) including \"fish\", \"green\" and \"sea\" are more likely to be said by Melville than Austen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main character names\n",
    "#### Some of you are probably thinking: the classifier must be getting a lot of help from the main character names such as Emma, Ahab and Queequeg. Let's see how well it does without them. The script already contains a switch that you can turn on to \"neutralize\" the top 35 most common character names and place names in the two novels by turning them all into 'MontyPython'. Edit the file and set the value of noCharNames to True. Re-run the script. How is the new classifier's performance? Did it degrade as much as you expected? Why do you think that is? How is the top feature list affected? When you're done, set noCharNames back to False and re-build your classifier by running the script again. For the rest of this homework, USE THIS ORIGINAL SETTING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test data after neutralizing top 35 most common character names: 0.934\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on test data after neutralizing top 35 most common character names:\", nltk.classify.accuracy(whosaid, test_feats2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up 40 most informative features after neutralizing top 35 most common character names\n",
      "Most Informative Features\n",
      "           contains-miss = 1              austen : melvil =    385.9 : 1.0\n",
      "           contains-thou = 1              melvil : austen =    103.1 : 1.0\n",
      "        contains-captain = 1              melvil : austen =     86.7 : 1.0\n",
      "            contains-mrs = 1              austen : melvil =     67.2 : 1.0\n",
      "          contains-smith = 1              austen : melvil =     64.0 : 1.0\n",
      "             contains-ye = 1              melvil : austen =     59.4 : 1.0\n",
      "         contains-martin = 1              austen : melvil =     58.0 : 1.0\n",
      "           contains-feet = 1              melvil : austen =     50.6 : 1.0\n",
      "       contains-isabella = 1              austen : melvil =     49.6 : 1.0\n",
      "           contains-fish = 1              melvil : austen =     42.3 : 1.0\n",
      "      contains-agreeable = 1              austen : melvil =     41.1 : 1.0\n",
      "           contains-dear = 1              austen : melvil =     40.0 : 1.0\n",
      "        contains-herself = 1              austen : melvil =     38.7 : 1.0\n",
      "           contains-iron = 1              melvil : austen =     38.0 : 1.0\n",
      "        contains-manners = 1              austen : melvil =     37.7 : 1.0\n",
      "      contains-happiness = 1              austen : melvil =     33.8 : 1.0\n",
      "             contains-\"' = 1              melvil : austen =     33.3 : 1.0\n",
      "         contains-nobody = 1              austen : melvil =     33.3 : 1.0\n",
      "         contains-waters = 1              melvil : austen =     30.7 : 1.0\n",
      "     contains-delightful = 1              austen : melvil =     30.1 : 1.0\n",
      "       contains-carriage = 1              austen : melvil =     29.7 : 1.0\n",
      "       contains-daughter = 1              austen : melvil =     29.7 : 1.0\n",
      "            contains-thy = 1              melvil : austen =     28.2 : 1.0\n",
      "   contains-acquaintance = 1              austen : melvil =     27.7 : 1.0\n",
      "             contains-st = 1              melvil : austen =     27.5 : 1.0\n",
      "       contains-pleasure = 1              austen : melvil =     27.5 : 1.0\n",
      "           contains-.--\" = 1              austen : melvil =     26.7 : 1.0\n",
      "        contains-further = 1              melvil : austen =     26.0 : 1.0\n",
      "           contains-dead = 1              melvil : austen =     25.7 : 1.0\n",
      "      contains-delighted = 1              austen : melvil =     24.1 : 1.0\n",
      "          contains-marry = 1              austen : melvil =     24.1 : 1.0\n",
      "             contains-,) = 1              austen : melvil =     24.1 : 1.0\n",
      "            contains-sea = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-green = 1              melvil : austen =     23.9 : 1.0\n",
      "          contains-maple = 1              austen : melvil =     22.5 : 1.0\n",
      "      contains-gratitude = 1              austen : melvil =     22.5 : 1.0\n",
      "         contains-robert = 1              austen : melvil =     22.5 : 1.0\n",
      "          contains-along = 1              melvil : austen =     22.4 : 1.0\n",
      "         contains-vessel = 1              melvil : austen =     22.3 : 1.0\n",
      "           contains-bone = 1              melvil : austen =     22.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Looking up 40 most informative features after neutralizing top 35 most common character names\")\n",
    "whosaid2.show_most_informative_features(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original accuracy score was 0.942 but it dropped to 0.934 after neutralizing the top 35 most common character names. In my opinion, this drop in accuracy score is understandable because whether certain main character names appear in the text or not is pretty influential in the classification process. For example, \"Perry\" was one of the top40 significant features for the first model where we set noCharNames as False. In that model, \"Perry\" was 24 times more likely to appear in Austin's speech than that of Melville. After setting noCharNames as True, the \"Perry\" feature was no longer was included as one of the features in the model which led to a slightly decrease in accuracy score. However, the top feature list hasn't been affected as much and the same pattern mentioned in Question 2 still holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trying out sentences\n",
    "#### Test the classifier on the two sentences below. Sent1 is actually by Jane Austen, taken from Persuasion. Sent2 is from Alice's Adventures in Wonderland by Lewis Caroll.\n",
    "- (Sent1) Anne was to leave them on the morrow, an event which they all dreaded.\n",
    "- (Sent2) So Alice began telling them her adventures from the time when she first saw the White Rabbit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Anne was to leave them on the morrow, an event which they all dreaded.\"\n",
    "sent2 = \"So Alice began telling them her adventures from the time when she first saw the White Rabbit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sent_feat(sentence):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        features['contains-{}'.format(word.lower())] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_feat = gen_sent_feat(sent1)\n",
    "sent2_feat = gen_sent_feat(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austen'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.classify(sent1_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'melville'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.classify(sent2_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent1 which we know is from Austen was classified as Austen, so the classifier seems to be working fine. sent2 was classified as \"melville\" which makes sense because sent2 includes words like \"White\", \"Rabbit\" which is related to the pattern we found earlier that sentences that include words representing solid entities (e.g. animals, nature stuff, objects etc.) are more likely to appear in melville's text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Label probabilities for a sentence\n",
    "\n",
    "#### Labeling judgments aside, how likely does your model thinks that Sent1 is Austen? That is essentially P(austen|Sent1). To find out, we need to use the .prob_classify method instead of the usual .classify. Below demonstrates how to find the probability estimates assigned to eithe****r label for the sentence 'Hello, world'. whosaid thinks it's 72% Melville and 28% Austen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** a. Try it with Sent1. What is P(austen|Sent1)? That is, given Sent1, how likely is it to be Austen? What is P(melville|Sent1)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9567679967331554 0.04323200326684133\n"
     ]
    }
   ],
   "source": [
    "print(whosaid.prob_classify(sent1_feat).prob('austen'), whosaid.prob_classify(sent1_feat).prob('melville'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(austen|Sent1) = 95.6% and P(melville|Sent1) = 4.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. How about Sent2 -- P(austen|Sent2) and P(melville|Sent2)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4385996907223784 0.5614003092776179\n"
     ]
    }
   ],
   "source": [
    "print(whosaid.prob_classify(sent2_feat).prob('austen'), whosaid.prob_classify(sent2_feat).prob('melville'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(austen|Sent2) = 43.9% and P(melville|Sent2) = 56.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. From a. and b., how \"confident\" is your classifier about Sent1 being Austen? Is it equally confident on Sent2 being Melville?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No! The classifier is very confident about sent1 being Austen as we can see from the very high conditional probability of 95.6%. On the other hand, the classifier is less confident about sent1 being Melville than sent1 being Austen as we can see from the conditional probability of 56.1% although this is still higher than the conditional probability of sent2 being Austen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trying out made-up sentences\n",
    "Now, test the classifier on the following made-up sentences:\n",
    "\n",
    "- (Sent3) He knows the truth\n",
    "- (Sent4) She knows the truth\n",
    "- (Sent5) blahblahblah blahblah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = 'He knows the truth'\n",
    "sent4 = 'She knows the truth'\n",
    "sent5 = 'blahblahblah blahblah'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What labels did the classifier give to Sent3 and Sent4, and with what probabilities? Any thoughts?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified label of sent3:  melville\n",
      "Classified label of sent4:  austen\n",
      "Conditional Probs for sent3 (austen|sent3 & melville|sent3):  0.4910301810185188 0.5089698189814804\n",
      "Conditional Probs for sent4 (austen|sent4 & melville|sent4):  0.9383515139571844 0.061648486042814823\n"
     ]
    }
   ],
   "source": [
    "sent3_feat = gen_sent_feat(sent3)\n",
    "sent4_feat = gen_sent_feat(sent4)\n",
    "\n",
    "print(\"Classified label of sent3: \", whosaid.classify(sent3_feat))\n",
    "print(\"Classified label of sent4: \", whosaid.classify(sent4_feat))\n",
    "\n",
    "print(\"Conditional Probs for sent3 (austen|sent3 & melville|sent3): \",\n",
    "      whosaid.prob_classify(sent3_feat).prob('austen'), whosaid.prob_classify(sent3_feat).prob('melville'))\n",
    "print(\"Conditional Probs for sent4 (austen|sent4 & melville|sent4): \",\n",
    "      whosaid.prob_classify(sent4_feat).prob('austen'), whosaid.prob_classify(sent4_feat).prob('melville'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense because we have previously found out from the top40 features that melville is closely associated with masculine terms such as captain/him/his while austen is more closely associated with feminine terms such as \"miss/she/her\". We also observe that the conditional probability of sent4 being austen is overwhelmingly high (93.8%) and this illustrates that feminine words such as \"she\" has an immense influence in classifying whether a sentence is Austen or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What about Sent5? Given that neither \"word\" appeared in the training data, why do you think the classifier made the prediction it did?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified label of sent5:  melville\n",
      "Conditioanl Probs for sent5 (austen|sent5 & melville|sent5):  0.44034184649904307 0.5596581535009568\n"
     ]
    }
   ],
   "source": [
    "sent5_feat = gen_sent_feat(sent5)\n",
    "\n",
    "print(\"Classified label of sent5: \", whosaid.classify(sent5_feat))\n",
    "\n",
    "print(\"Conditioanl Probs for sent5 (austen|sent5 & melville|sent5): \",\n",
    "      whosaid.prob_classify(sent5_feat).prob('austen'), whosaid.prob_classify(sent5_feat).prob('melville'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am assuming that we had more sentences coming from melville in the corpus which made the probability be more heavily weighted towards melville."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Base probabilities (= priors)\n",
    "Not knowing anything about a sentence, is it more likely to be Austen or Melville? We can answer this question by establishing the base probabilities, i.e. priors. In your training data (i.e., train_sents or train_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. How many sentences are there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15152 sentences.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} sentences.\".format(len(train_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. How many of them are Austen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen = []\n",
    "melville = []\n",
    "for s in train_sents:\n",
    "    if s[1]=='austen':\n",
    "        austen.append(s)\n",
    "    elif s[1] == 'melville':\n",
    "        melville.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6672 sentences from Austen.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} sentences from Austen.\".format(len(austen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. How many of them are Melville?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8480 sentences from Austen.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} sentences from Austen.\".format(len(melville)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. From the above, what are P(austen) and P(melville)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(austen):  0.440337909186906\n",
      "P(melville):  0.5596620908130939\n"
     ]
    }
   ],
   "source": [
    "print(\"P(austen): \", len(austen) / len(train_sents))\n",
    "print(\"P(melville): \", len(melville) / len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. How is your answer to d. related to the classifier's prediction on Sent5 above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probabilities exactly match with the conditional probabilities of P(austen|sent5) and P(melville|sent5) and this shows that prior probabilities which are weighted depending on how many sentences from melville or austen appear in the corpus have been used for sentences like sent5 where neither of the two words is in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculating odds ratio\n",
    "Would the word 'very' be more indicative of Austen or Melville, and how strongly so? Let's answer this by calculating its odds ratio. Find out the following, again in the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. How many Austen sentences contain 'very'? Make sure to count 'Very' as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_very_count = 0\n",
    "for s in austen:\n",
    "    if 'very' in s[0]:\n",
    "        austen_very_count += 1\n",
    "    elif 'Very' in s[0]:\n",
    "        austen_very_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934 Austen sentences contain the word 'very'.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} Austen sentences contain the word 'very'.\".format(austen_very_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.How about Melville sentences?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "melville_very_count = 0\n",
    "for s in melville:\n",
    "    if 'very' in s[0]:\n",
    "        melville_very_count += 1\n",
    "    elif 'Very' in s[0]:\n",
    "        melville_very_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 Melville sentences contain the word 'very'.\n"
     ]
    }
   ],
   "source": [
    "print(\"{} Melville sentences contain the word 'very'.\".format(melville_very_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. What is P(very|austen)? That is, given an Austen sentence, how likely is it to contain 'very'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(very|austen):  0.13998800959232613\n"
     ]
    }
   ],
   "source": [
    "print(\"P(very|austen): \", austen_very_count / len(austen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. What is P(very|melville)? That is, given a Melville sentence, how likely is it to contain 'very'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(very|melville):  0.03195754716981132\n"
     ]
    }
   ],
   "source": [
    "print(\"P(very|melville): \", melville_very_count / len(melville))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. What is Austen-to-Melville odds ratio of 'very'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen to Melville odds ratio is:  4.380436610121497\n"
     ]
    }
   ],
   "source": [
    "print(\"Austen to Melville odds ratio is: \",(austen_very_count / len(austen)) / (melville_very_count / len(melville)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature weights in model\n",
    "P(very|austen) and P(very|melville) are indeed the 'weights' your model assigns to the feature 'contains-very':1. Let's confirm this by probing your model. Use the .feature_weights() method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What are the weights of 'very'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.14004196013786901, 'melville': 0.0320127343473647}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-very', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Do they match up with what you calculated in 8.c and 8.d above? (They should. The small differences are effects of smoothing, which may be more pronounced in other cases.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 0.13998800959232613 and 0.03195754716981132 for 8.c and 8.d and they almost match with the values we got from the feature_weights (0.14, 0.032) with some minor differences due to smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Zero counts and feature weights\n",
    "In order to accommodate features and feature-value pairs never encountered in the training data, a machine learning algorithm will adopt a couple of strategies, including smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Look up the feature weights of 'contains-whale' and also 'contains-ahab'. What do you notice?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 7.49288176232579e-05, 'melville': 0.11266360099044924}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-whale', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 7.49288176232579e-05, 'melville': 0.05017097040443344}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-ahab', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that the weight of the two words for austen is exactly the same while the weight of \"whale\" for melville is higher than that of \"ahab\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. This time, look up the feature weights for words 'marriage' and 'Emma'. Anything noticeable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.004870373145511764, 'melville': 0.00029477655936799903}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-marriage', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.10962086018282631, 'melville': 5.895531187359981e-05}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-emma', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that the feature weight of \"marriage\" is more than 10 times higher for austen than for melville. On the flip side, I notice that the feature weight of \"emma\" for austen is more ethan 1859 times higher than for melville. The words \"marriage\" and \"emma\" are very highly important in determining whether a sentence is classifed as Austen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Find a word that occurs in Austen's work only, and another that occurs only in Melville, and then look up their feature weights. You should have a theory by now -- sum up what is going on with these words and their feature weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_Jane_count = 0\n",
    "for s in austen:\n",
    "    if 'Jane' in s[0]:\n",
    "        austen_Jane_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03821942446043165"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jane feature weight for Austen\n",
    "austen_Jane_count / len(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "melville_captain_count = 0\n",
    "for s in melville:\n",
    "    if 'captain' in s[0]:\n",
    "         melville_captain_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01179245283018868"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# captain feature weight for Melville\n",
    "melville_captain_count / len(melville)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Jane\" has a pretty high feature weight for Austen of 0.038 considering the low feature weights can be something as low as some integer times 10 to the power of -5. Similarly, the feature weight of \"captain\" for Melville is 0.0117 which is also pretty high. These high values of feature weight shows that \"Jane\" and \"captain\" are key words that can be used to determine whether a sentence is classified as Austen or Melville respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. As a comparison point, the word 'cautiously' occurs exactly once in the Austen training sentences, and likewise in the Melville training sentences. How do its feature weights compare against the ones you saw above?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.0002247864528697737, 'melville': 0.00017686593562079943}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-cautiously', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature weights of \"cautiously\" for Austen and Melville are both about 10 times lower than \"Jane\" and \"captain\" respectively. This means the likelihood of the word \"cautiously\" appearing in Austen and Melville text is lower than \"Jane\" and \"captain\" respectively. This makes sense considering how \"Jane\" and \"captain\" are unique words that only appear in one text or the other while \"cautiously\" can appear in both texts (although in varying degrees of frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Now, try 'contains-internet'. What happens this time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('melville', 'contains-internet')\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('ftpuploader')\n",
    "\n",
    "try:\n",
    "    whosaid.feature_weights('contains-internet', 1)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that none of the sentences from Melville contains the word \"internet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_internet_count = 0\n",
    "for s in austen:\n",
    "    if 'internet' in s[0]:\n",
    "        austen_internet_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# internet feature weight for Austen\n",
    "austen_internet_count / len(austen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oops! We see that none of the sentences from Austen contains the word \"internet\" as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Then, using the .prob_classify method, find out the likelihood of 'She hates the internet' being an Austen sentence. Then try 'She hates the'. What can you conclude about the classifier's handling of features it never encountered in the training data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent7 = 'She hates the internet'\n",
    "sent8 = 'She hates the'\n",
    "sent7_feat = gen_sent_feat(sent7)\n",
    "sent8_feat = gen_sent_feat(sent8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958225408422634 0.8958225408422634\n"
     ]
    }
   ],
   "source": [
    "print(whosaid.prob_classify(sent7_feat).prob('austen'), whosaid.prob_classify(sent8_feat).prob('austen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the probabilities of sent7 and sent8 being classifed as Austen are the same because the word \"internet\" is not in the training data. The classifier ignores words like \"internet\" that are not part of the training data and only uses other words that appear in the training data to calculate conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Combining feature weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. You already calculated the Austen prior: P(austen) in 7 d. What is it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(austen) is 0.440337909186906."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. P(he|austen) can be found through whosaid.feature_weights('contains-he', 1). Likewise with the rest of the words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.16776562265847444, 'melville': 0.15334276618323311}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-he', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.004270942604525701, 'melville': 0.0028888102818063906}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-knows', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.3799640341675408, 'melville': 0.6001061195613725}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-the', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austen': 0.004870373145511764, 'melville': 0.004067916519278387}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whosaid.feature_weights('contains-truth', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. From a. and b., calculate P(Sent3, austen)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sent3_aust = 0.440337909186906 * 0.16776562265847444 * 0.004270942604525701 * 0.3799640341675408 * 0.004870373145511764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sent3, austen) = P(austen) * P(austen|he) * P(austen|knows) * P(austen|the) * P(austen|truth) = 5.838718138066814e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"P(sent3, austen) = P(austen) * P(austen|he) * P(austen|knows) * P(austen|the) * P(austen|truth) = {}\".\\\n",
    "      format(0.440337909186906 * 0.16776562265847444 * 0.004270942604525701 * 0.3799640341675408 * 0.004870373145511764))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. Similarly, calculate P(Sent3, melville).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sent3_melv = 0.5596620908130939 * 0.15334276618323311 * 0.0028888102818063906 * 0.6001061195613725 * 0.004067916519278387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sent3, melville) = P(melville) * P(melville|he) * P(melville|knows) * P(melville|the) * P(melville|truth) = 6.052130617577441e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"P(sent3, melville) = P(melville) * P(melville|he) * P(melville|knows) * P(melville|the) * P(melville|truth) = {}\".\\\n",
    "      format(0.5596620908130939 * 0.15334276618323311 * 0.0028888102818063906 * 0.6001061195613725 * 0.004067916519278387))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Now, calculate P(Sent3) as P(Sent3, austen) + P(Sent3, melville).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(sent3) = P(sent3, austen) + P(sent3, melville) = 1.1890848755644255e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"P(sent3) = P(sent3, austen) + P(sent3, melville) = {}\".format(p_sent3_aust + p_sent3_melv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. Ultimately, the probability question we like to answer is: \"Given the sentence He knows the truth, how likely is it to be Austen\"? That is, what is P(austen|Sent3)? Use formula ① above to calculate this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(austen|sent3) = 0.4910261881260021\n"
     ]
    }
   ],
   "source": [
    "print(\"P(austen|sent3) = {}\".format(p_sent3_aust / (p_sent3_aust + p_sent3_melv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. Does the figure match up with the classifier's estimation from 6.a above? (It should.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(austen|sent3) calculated in 6.a was 0.4910301810185188 which matches with the P(austen|sent3) calculated in (f) with a minor difference in the later parts of the decimals which can be considered as an acceptable difference within the margion of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance on the development-test data\n",
    "Work with the four lists aa, mm, am, ma to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Of the 1,000 development-test set, how many of them did the classifier correctly label?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957 out of 1000 development test set were correctly labeled\n"
     ]
    }
   ],
   "source": [
    "print(\"{} out of 1000 development test set were correctly labeled\".format(len(aa) + len(mm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What is whosaid's accuracy on the development test data? Ideally, it should be close to its performance on the test data -- is it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whosaid's accuracy on the development-test data is 957 / 1000 = 0.957 while the the accuracy score on the test data was 0.942 as we have seen in step10 of Part(A). The accuracy score on the development test data isn't necessarily close to that on the test data especially when the model overfitted and hence is not good at classifying new unseen test data. If the accuracy on the development-test data gets closer to the performance on the test data, this means the model is moving towards a direction where variance is being reduced so that its predictive power for unseen data increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. What % of the sentences did the classifier label as 'austen'? How about 'melville'? Why do you think it is not 50-50?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.7, 54.3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(ma) + len(aa)) * 100 / 1000, (len(am) + len(mm)) * 100 / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45.7% of sentences were labelled as austen and 54.3% of the sentences were labelled as melville. The classifier might not have (and often time do not have) the same predictive power for each of the labels (in this case, austen and melville). For example, if the classifier does a poor job at correctly classifying austen sentences, it might classify more sentences as melville and this will cause the number of sentences labelled as melville to blow up. This is why it is very rare to achieve 50-50 in terms of the % of the sentences the classifier label as austen and melville."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. What % of the classifier's 'austen' rulings is correct? That is, when the classifier labels a sentence as 'Austen', what is the likelihood of this prediction to be correct?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9562363238512035"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa) / (len(ma) + len(aa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the classifier labels a sentence as Austen, the likelihood of this prediction to be correct is about 95.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Likewise, when the classifier labels a sentence as 'melville', what is the likelihood of this prediction being correct?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9576427255985267"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mm) / (len(mm) + len(am))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the classifier labels a sentence as melville, the likelihood of this prediction to be correct is about 95.76%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Error analysis\n",
    "The list am contains all sentences from the dev-test set that are in fact Austen but were mis-labeled as Melville by whosaid. Let's take a look at these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Print out all mis-classified Austen sentences by: for x in am: print(' '.join(x[2]))**\n",
    "\n",
    "**What do you think of these sentences? Do they sound Melville-like to you?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Dating from three o ' clock yesterday .\n",
      "or a mermaid ?\n",
      "It was badly done , indeed !\n",
      "\" Well -- as you please ; only don ' t have a great set out .\n",
      "He came four times a day for a week .\n",
      "The heat overcame me .\"\n",
      "They are not far off .\n",
      "But fetch them both .\n",
      "The gentlemen spoke of his horse .\n",
      "Lord of the earth and sea , he bends a slave , And woman , lovely woman , reigns alone .\n",
      "It might be safely viewed with all its appendages of prosperity and beauty , its rich pastures , spreading flocks , orchard in blossom , and light column of smoke ascending .-- She joined them at the wall , and found them more engaged in talking than in looking around .\n",
      "\" Good God !\"\n",
      "They are ripening fast .\"\n",
      "In a moment he went on --\n",
      "What an air of probability sometimes runs through a dream !\n",
      "\" Good God !\"\n",
      "that ' s all .\n",
      "He had gone beyond the sweep -- some way along the Highbury road -- the snow was nowhere above half an inch deep -- in many places hardly enough to whiten the ground ; a very few flakes were falling at present , but the clouds were parting , and there was every appearance of its being soon over .\n",
      "But hush , hush !\"\n",
      "It used to stand here .\n",
      "how so ?\n",
      "He bowed .\n",
      "\" What !\n"
     ]
    }
   ],
   "source": [
    "for x in am: \n",
    "    print(' '.join(x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the sentences sound like Melville to me because of words like \"ground\", \"sea\", \"earth\", \"He\" and \"mermaid\" that represent tangible entities and masculinity. Earlier we noticed that such words are more likely to appear in Melville's text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. When a classifier mislabels, it is hoped that it at least did so with low confidence (say, 55%) than high (98%). Pick some sentence from the list and see what likelihood whosaid assigned to them for being Melville. Of all sentences you tried, which was judged Melville with the lowest confidence? Which one was it most sure about being Melville?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('austen', 'melville', ['\"', 'Dating', 'from', 'three', 'o', \"'\", 'clock', 'yesterday', '.']) 0.9997479147589855\n",
      "('austen', 'melville', ['or', 'a', 'mermaid', '?']) 0.999221365737171\n",
      "('austen', 'melville', ['It', 'was', ('austen', 'melville', ['\"', 'Dating', 'from', 'three', 'o', \"'\", 'clock', 'yesterday', '.']), 'done', ',', 'indeed', '!']) 0.999981457560398\n",
      "('austen', 'melville', ['\"', 'Well', '--', 'as', 'you', 'please', ';', 'only', 'don', \"'\", 't', 'have', 'a', 'great', 'set', 'out', '.']) 0.999999866164424\n",
      "('austen', 'melville', ['He', 'came', 'four', 'times', 'a', 'day', 'for', 'a', 'week', '.']) 0.9940807645151838\n",
      "('austen', 'melville', ['The', 'heat', 'overcame', 'me', '.\"']) 0.9999191922846851\n",
      "('austen', 'melville', ['They', 'are', 'not', 'far', 'off', '.']) 0.9994441003426138\n",
      "('austen', 'melville', ['But', 'fetch', 'them', 'both', '.']) 0.9969821026446707\n",
      "('austen', 'melville', ['The', 'gentlemen', 'spoke', 'of', 'his', 'horse', '.']) 0.9997983060281489\n",
      "('austen', 'melville', ['Lord', 'of', 'the', 'earth', 'and', 'sea', ',', 'he', 'bends', 'a', 'slave', ',', 'And', 'woman', ',', 'lovely', 'woman', ',', 'reigns', 'alone', '.']) 0.999991906157302\n",
      "('austen', 'melville', ['It', 'might', 'be', 'safely', 'viewed', 'with', 'all', 'its', 'appendages', 'of', 'prosperity', 'and', 'beauty', ',', 'its', 'rich', 'pastures', ',', 'spreading', 'flocks', ',', 'orchard', 'in', 'blossom', ',', 'and', 'light', 'column', 'of', 'smoke', 'ascending', '.--', 'She', 'joined', 'them', 'at', 'the', 'wall', ',', 'and', 'found', 'them', 'more', 'engaged', 'in', 'talking', 'than', 'in', 'looking', 'around', '.']) 0.999998808915152\n",
      "('austen', 'melville', ['\"', 'Good', 'God', '!\"']) 0.9929955321496763\n",
      "('austen', 'melville', ['They', 'are', 'ripening', 'fast', '.\"']) 0.9995141010997778\n",
      "('austen', 'melville', ['In', 'a', 'moment', 'he', 'went', 'on', '--']) 0.9998506455097518\n",
      "('austen', 'melville', ['What', 'an', 'air', 'of', 'probability', 'sometimes', 'runs', 'through', 'a', 'dream', '!']) 0.9999941917637667\n",
      "('austen', 'melville', ['\"', 'Good', 'God', '!\"']) 0.9929955321496763\n",
      "('austen', 'melville', ['that', \"'\", 's', 'all', '.']) 0.9914773064084914\n",
      "('austen', 'melville', ['He', 'had', 'gone', 'beyond', 'the', 'sweep', '--', 'some', 'way', 'along', 'the', 'Highbury', 'road', '--', 'the', 'snow', 'was', 'nowhere', 'above', 'half', 'an', 'inch', 'deep', '--', 'in', 'many', 'places', 'hardly', 'enough', 'to', 'whiten', 'the', 'ground', ';', 'a', 'very', 'few', 'flakes', 'were', 'falling', 'at', 'present', ',', 'but', 'the', 'clouds', 'were', 'parting', ',', 'and', 'there', 'was', 'every', 'appearance', 'of', 'its', 'being', 'soon', 'over', '.']) 0.9999962632000773\n",
      "('austen', 'melville', ['But', 'hush', ',', 'hush', '!\"']) 0.9982027763459203\n",
      "('austen', 'melville', ['It', 'used', 'to', 'stand', 'here', '.']) 0.9999839965193636\n",
      "('austen', 'melville', ['how', 'so', '?']) 0.9539807264678665\n",
      "('austen', 'melville', ['He', 'bowed', '.']) 0.9781712160983701\n",
      "('austen', 'melville', ['\"', 'What', '!']) 0.9396440965916052\n"
     ]
    }
   ],
   "source": [
    "for s in am:\n",
    "    print(s, whosaid.prob_classify(gen_feats(str(s[2]))).prob('melville'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sentence **['It', 'might', 'be', 'safely', 'viewed', 'with', 'all', 'its', 'appendages', 'of', 'prosperity', 'and', 'beauty', ',', 'its', 'rich', 'pastures', ',', 'spreading', 'flocks', ',', 'orchard', 'in', 'blossom', ',', 'and', 'light', 'column', 'of', 'smoke', 'ascending', '.--', 'She', 'joined', 'them', 'at', 'the', 'wall', ',', 'and', 'found', 'them', 'more', 'engaged', 'in', 'talking', 'than', 'in', 'looking', 'around', '.']** had a confidence level of 0.999998808915152 and thus it was what the classifier was most sure about being melville (wrongly classified with the most confidence)\n",
    "\n",
    "- **['\"', 'What', '!']** had a confidence level of 0.9396440965916052 and thus it was what the classifier was least sure about being melville"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
